{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfa58db0",
   "metadata": {},
   "source": [
    "# Text Classification Model Analysis\n",
    "\n",
    "In this assignment, we aim to explore and compare different machine learning models for a text classification task. Our objective is to identify the most effective model based on performance metrics such as accuracy, precision, and recall. This notebook encompasses the entire workflow from data preprocessing, model training, hyperparameter tuning, to the final comparison and selection of the best model.\n",
    "\n",
    "### Problem statement, dataset, and objective of the analysis\n",
    "\n",
    "This assignment fosuses on text classification for cancer related documents. The selected dataset contains 7569 documents, which are distributed across 3 cancer types: thyroid, colon and lung. The purposed of the analysis is to conduct text analysis and build machine learning models to accurately identify the type of cancer on each observation. \n",
    "\n",
    "The dataset can be downloaded from the following source: https://www.kaggle.com/datasets/falgunipatel19/biomedical-text-publication-classification/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f6aa81",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "\n",
    "### Data Preprocessing Steps:\n",
    "\n",
    "- **Loading and Understanding the Data**:performing exploratory data analysis to understand the structure, content, and any immediate data quality issues.\n",
    "- **Cleaning Data**: Removing special characters, numbers, and unnecessary whitespace.\n",
    "- **Tokenization**: Splitting text into individual words or tokens.\n",
    "- **Lemmatization/Stemming**: Reducing words to their base or root form.\n",
    "- **Stop Word Removal**: Eliminating common words that provide little value in the analysis.\n",
    "- **Vectorization (TF-IDF)**: Converting text into numeric form to create feature vectors.\n",
    "- **Feature Engineering**:  Encode the text data using sklearn's TfidfVectorizer to convert text data into a matrix of TF-IDF features.\n",
    "- **Splitting the Data**: The data is divided into training and testing sets to evaluate the model's performance on unseen data. \n",
    "\n",
    "### Rationale Behind Chosen Methods:\n",
    "\n",
    "- **Cleaning Data**: Ensures data quality and consistency, which are fundamental for reliable model predictions.\n",
    "- **Feature Engineering**: Leverages domain knowledge to introduce new relevant features, potentially improving model performance by providing more informative signals.\n",
    "- **Text Processing**: Text data must be converted into a numerical format for machine learning algorithms to process it. This step is crucial for any task involving text analysis.\n",
    "- **Normalization/Standardization**: Helps to equalize the influence of features on the model's outcome, improving training stability and performance.\n",
    "- **Splitting the Data**: Essential for validating the model's ability to generalize from the training data to unseen data, providing an estimate of its performance in real-world scenarios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2da4f711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing #for preprocessing text data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #TfidfVectorizer (which includes pre-processing, tokenization, and filtering out stop words)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from string import punctuation\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53a0c215-d17d-4ff3-95d8-f10807837b00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Thyroid surgery in  children in a single insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>\" The adopted strategy was the same as that us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis ï¬b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Solitary plasmacytoma SP of the skull is an u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>This study aimed to investigate serum matrix ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0               0  \\\n",
       "0           0  Thyroid_Cancer   \n",
       "1           1  Thyroid_Cancer   \n",
       "2           2  Thyroid_Cancer   \n",
       "3           3  Thyroid_Cancer   \n",
       "4           4  Thyroid_Cancer   \n",
       "\n",
       "                                                   a  \n",
       "0  Thyroid surgery in  children in a single insti...  \n",
       "1  \" The adopted strategy was the same as that us...  \n",
       "2  coronary arterybypass grafting thrombosis ï¬b...  \n",
       "3   Solitary plasmacytoma SP of the skull is an u...  \n",
       "4   This study aimed to investigate serum matrix ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Cancer_Dataset.csv\", encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b76f2cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class_Labels</th>\n",
       "      <th>Research_Paper_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Thyroid surgery in  children in a single insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>\" The adopted strategy was the same as that us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis ï¬b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>Solitary plasmacytoma SP of the skull is an u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>This study aimed to investigate serum matrix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7565</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>we report the case of a 24yearold man who pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7566</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>among synchronous colorectal cancers scrcs rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7567</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the heterogeneity of cancer cells is generally...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7568</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>\"adipogenesis is the process through which mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7569</th>\n",
       "      <td>Colon_Cancer</td>\n",
       "      <td>the periparturient period is one of the most c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Class_Labels                                Research_Paper_Text\n",
       "0     Thyroid_Cancer  Thyroid surgery in  children in a single insti...\n",
       "1     Thyroid_Cancer  \" The adopted strategy was the same as that us...\n",
       "2     Thyroid_Cancer  coronary arterybypass grafting thrombosis ï¬b...\n",
       "3     Thyroid_Cancer   Solitary plasmacytoma SP of the skull is an u...\n",
       "4     Thyroid_Cancer   This study aimed to investigate serum matrix ...\n",
       "...              ...                                                ...\n",
       "7565    Colon_Cancer  we report the case of a 24yearold man who pres...\n",
       "7566    Colon_Cancer  among synchronous colorectal cancers scrcs rep...\n",
       "7567    Colon_Cancer  the heterogeneity of cancer cells is generally...\n",
       "7568    Colon_Cancer  \"adipogenesis is the process through which mes...\n",
       "7569    Colon_Cancer  the periparturient period is one of the most c...\n",
       "\n",
       "[7570 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dropping the irrelevant column\n",
    "df = df.drop('Unnamed: 0',axis=1)\n",
    "# Renaming the column names\n",
    "df.columns=['Class_Labels', 'Research_Paper_Text']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b79b978c-ab06-4deb-9158-4071ce06947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Null Values\n",
    "count = df['Class_Labels'].isna().sum()\n",
    "if  count > 0:\n",
    "    print(f'Found {count} null values in Class_Labels column')\n",
    "    #df['Class_Labels'].fillna('missing', inplace=True) # though we could do this, we will drop the rows instead - as there is no way to impute the text\n",
    "    df = df.dropna(subset=['Class_Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8afbf713-97ec-4d21-98d3-28d5334941fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping Null Values\n",
    "count = df['Research_Paper_Text'].isna().sum()\n",
    "if  count > 0:\n",
    "    print(f'Found {count} null values in Research_Paper_Text column')\n",
    "    #df['Research_Paper_Text'].fillna('missing', inplace=True) # though we could do this, we will drop the rows instead - as there is no way to impute the text\n",
    "    df = df.dropna(subset=['Research_Paper_Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3352787f-5820-48d1-b071-32072ad796f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Thyroid_Cancer', 'Colon_Cancer', 'Lung_Cancer'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Class_Labels'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e58f4fba-dfab-4d14-a23f-afda79653bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thyroid_Cancer    2810\n",
       "Colon_Cancer      2580\n",
       "Lung_Cancer       2180\n",
       "Name: Class_Labels, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking data imbalance\n",
    "df['Class_Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "913e5abe-37dd-4117-a0c0-c5c6d268b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define stopwords list\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define function for text cleaning and lemmatization\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Cleaning and lemmatization\n",
    "    clean_lemmatized_text = [lemmatizer.lemmatize(token.lower()) for token in tokens if (token.lower() not in punctuation) and (token.lower() not in stopwords_list) and (len(token) > 2) and token.isalpha()]\n",
    "    return \" \".join(clean_lemmatized_text)\n",
    "\n",
    "# Apply text preprocessing to the 'Research_Paper_Text' column\n",
    "df['Research_Paper_Text'] = df['Research_Paper_Text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5bf007f7-ca07-4fa9-bc8e-8f41609bcc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class_Labels</th>\n",
       "      <th>Research_Paper_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>thyroid surgery child single institution osama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>adopted strategy used prior year based four ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>coronary arterybypass grafting thrombosis muta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>solitary plasmacytoma skull uncommon clinical ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thyroid_Cancer</td>\n",
       "      <td>study aimed investigate serum matrix metallopr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Class_Labels                                Research_Paper_Text\n",
       "0  Thyroid_Cancer  thyroid surgery child single institution osama...\n",
       "1  Thyroid_Cancer  adopted strategy used prior year based four ex...\n",
       "2  Thyroid_Cancer  coronary arterybypass grafting thrombosis muta...\n",
       "3  Thyroid_Cancer  solitary plasmacytoma skull uncommon clinical ...\n",
       "4  Thyroid_Cancer  study aimed investigate serum matrix metallopr..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b4db9ca3-6b0d-484e-b1f6-13417346d18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Research_Paper_Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a76cb8e-ab8b-43f0-bc63-d0abfa790f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Class_Labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61d33994-082c-4264-b904-e5dac3aba81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'Colon_Cancer'), (1, 'Lung_Cancer'), (2, 'Thyroid_Cancer')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "classes = list(enumerate(le.classes_))\n",
    "print(classes)\n",
    "y = le.transform(y)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dc1c274d-9046-4b82-862f-46ded2bb60e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d8aee252-3f7d-463a-a368-07e2aa2e3975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5299,), (5299,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c580f345-49a8-491f-836f-d4ea1b3aabf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2271,), (2271,))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b49ae110-90d4-4c24-8394-a90fbef370ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1, ..., 1, 1, 2])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80047503-a29b-40da-9890-1dd8c1bbd3ab",
   "metadata": {},
   "source": [
    "Sklearn: Text preparation\n",
    "For simplicity (and focus), we will not do any text cleaning or preprocessing. We will just use the raw text as input to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8e07ede2-452e-4d70-a3df-95ee125ef901",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer() \n",
    "\n",
    "X_train = tfidf_vect.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac1cf727-8651-4a91-82ba-1a6e2608db0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5299, 146913)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "91313a30-9083-4855-ba4d-65c8c234b0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5299x146913 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4612930 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e32ad013-4694-45be-8343-569d2fe127c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 ... 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e337210-7262-4842-a95b-332a7c72d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the TfidfVectorizer transformation\n",
    "\n",
    "\n",
    "X_test = tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "997f932b-9b8e-47e0-ba6e-2a3fab997ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5299, 146913), (2271, 146913))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "eb50dabd-1666-49b1-aec4-907de1aa4df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# These data sets are \"sparse matrix\". We can't see them unless we convert using toarray()\n",
    "np.set_printoptions(precision=3)\n",
    "print(X_train.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17697994-56f0-41f7-8f10-2ab28332ad72",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (Singular Value Decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1dbfd93e-4b20-4760-b07e-0fe88966f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000, n_iter=10) #n_components is the number of topics, which should be less than the number of features, and number of rows in the matrix\n",
    "\n",
    "X_train_dim_reduct = svd.fit_transform(X_train)\n",
    "X_test__dim_reduct = svd.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24586354-d8db-4f0b-9ee5-ce3d46b43275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5299, 146913), (2271, 146913))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63d748f1-77bb-42b6-895a-5a6f28578566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5299, 1000), (2271, 1000))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dim_reduct.shape, X_test__dim_reduct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4842fb08-d920-47bb-978b-67b4118cfcac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.705e-01,  1.300e-01, -3.006e-02, ..., -1.256e-18, -3.663e-18,\n",
       "        -1.542e-18],\n",
       "       [ 1.672e-01, -1.148e-01,  9.938e-03, ..., -6.370e-19, -1.247e-18,\n",
       "        -1.965e-19],\n",
       "       [ 2.557e-01, -1.916e-01,  4.097e-02, ..., -2.819e-18, -1.708e-18,\n",
       "        -5.001e-18],\n",
       "       ...,\n",
       "       [ 2.344e-01,  1.359e-01, -7.398e-02, ...,  1.237e-18,  6.895e-19,\n",
       "         7.098e-19],\n",
       "       [ 2.948e-01, -5.169e-02,  3.892e-02, ..., -1.081e-18,  6.539e-19,\n",
       "         1.445e-18],\n",
       "       [ 2.183e-01, -2.331e-01, -7.712e-02, ...,  2.067e-19,  1.128e-18,\n",
       "         1.517e-18]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_dim_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d7f88f79-ffc3-47dc-a8db-2b952cb2942d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd0000</th>\n",
       "      <th>svd0001</th>\n",
       "      <th>svd0002</th>\n",
       "      <th>svd0003</th>\n",
       "      <th>svd0004</th>\n",
       "      <th>svd0005</th>\n",
       "      <th>svd0006</th>\n",
       "      <th>svd0007</th>\n",
       "      <th>svd0008</th>\n",
       "      <th>svd0009</th>\n",
       "      <th>...</th>\n",
       "      <th>svd0990</th>\n",
       "      <th>svd0991</th>\n",
       "      <th>svd0992</th>\n",
       "      <th>svd0993</th>\n",
       "      <th>svd0994</th>\n",
       "      <th>svd0995</th>\n",
       "      <th>svd0996</th>\n",
       "      <th>svd0997</th>\n",
       "      <th>svd0998</th>\n",
       "      <th>svd0999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.270461</td>\n",
       "      <td>0.130039</td>\n",
       "      <td>-0.030061</td>\n",
       "      <td>-0.004413</td>\n",
       "      <td>-0.012367</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>-0.021099</td>\n",
       "      <td>0.060785</td>\n",
       "      <td>0.083293</td>\n",
       "      <td>0.032068</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.253108e-18</td>\n",
       "      <td>-3.083200e-18</td>\n",
       "      <td>-2.683400e-18</td>\n",
       "      <td>3.001885e-18</td>\n",
       "      <td>4.065758e-19</td>\n",
       "      <td>-1.599198e-18</td>\n",
       "      <td>-3.781155e-18</td>\n",
       "      <td>-1.256150e-18</td>\n",
       "      <td>-3.662570e-18</td>\n",
       "      <td>-1.541600e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.167226</td>\n",
       "      <td>-0.114772</td>\n",
       "      <td>0.009938</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.059813</td>\n",
       "      <td>-0.013987</td>\n",
       "      <td>-0.020511</td>\n",
       "      <td>0.045512</td>\n",
       "      <td>-0.116298</td>\n",
       "      <td>...</td>\n",
       "      <td>3.794708e-19</td>\n",
       "      <td>-4.092863e-18</td>\n",
       "      <td>-1.694066e-18</td>\n",
       "      <td>1.497554e-18</td>\n",
       "      <td>4.252105e-19</td>\n",
       "      <td>1.355253e-20</td>\n",
       "      <td>-7.411538e-19</td>\n",
       "      <td>-6.369688e-19</td>\n",
       "      <td>-1.246832e-18</td>\n",
       "      <td>-1.965116e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.255663</td>\n",
       "      <td>-0.191578</td>\n",
       "      <td>0.040965</td>\n",
       "      <td>0.098437</td>\n",
       "      <td>-0.165433</td>\n",
       "      <td>0.070955</td>\n",
       "      <td>0.071593</td>\n",
       "      <td>0.018734</td>\n",
       "      <td>0.056828</td>\n",
       "      <td>-0.038015</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.195509e-18</td>\n",
       "      <td>-4.106416e-18</td>\n",
       "      <td>-5.204170e-18</td>\n",
       "      <td>-3.611748e-18</td>\n",
       "      <td>-6.308701e-18</td>\n",
       "      <td>-1.192622e-18</td>\n",
       "      <td>-3.601584e-18</td>\n",
       "      <td>-2.818926e-18</td>\n",
       "      <td>-1.707618e-18</td>\n",
       "      <td>-5.000883e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125376</td>\n",
       "      <td>-0.105049</td>\n",
       "      <td>-0.015376</td>\n",
       "      <td>-0.009029</td>\n",
       "      <td>-0.042645</td>\n",
       "      <td>-0.075272</td>\n",
       "      <td>0.026641</td>\n",
       "      <td>-0.000863</td>\n",
       "      <td>-0.028413</td>\n",
       "      <td>0.011569</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.867365e-18</td>\n",
       "      <td>-6.819462e-18</td>\n",
       "      <td>1.782157e-18</td>\n",
       "      <td>-5.746272e-18</td>\n",
       "      <td>-1.228198e-20</td>\n",
       "      <td>-7.559769e-18</td>\n",
       "      <td>1.314172e-18</td>\n",
       "      <td>5.655639e-18</td>\n",
       "      <td>2.422938e-18</td>\n",
       "      <td>3.423284e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.320904</td>\n",
       "      <td>0.160357</td>\n",
       "      <td>-0.002871</td>\n",
       "      <td>-0.014655</td>\n",
       "      <td>-0.034423</td>\n",
       "      <td>-0.044569</td>\n",
       "      <td>-0.028687</td>\n",
       "      <td>0.017214</td>\n",
       "      <td>0.078894</td>\n",
       "      <td>0.015232</td>\n",
       "      <td>...</td>\n",
       "      <td>1.450120e-18</td>\n",
       "      <td>2.317482e-18</td>\n",
       "      <td>1.246832e-18</td>\n",
       "      <td>6.606857e-19</td>\n",
       "      <td>-1.253609e-18</td>\n",
       "      <td>-1.463673e-18</td>\n",
       "      <td>5.739495e-18</td>\n",
       "      <td>1.057097e-18</td>\n",
       "      <td>-1.016440e-18</td>\n",
       "      <td>3.652406e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5294</th>\n",
       "      <td>0.225822</td>\n",
       "      <td>-0.005949</td>\n",
       "      <td>-0.020814</td>\n",
       "      <td>0.108820</td>\n",
       "      <td>-0.060078</td>\n",
       "      <td>-0.013237</td>\n",
       "      <td>-0.045673</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>0.089451</td>\n",
       "      <td>-0.059352</td>\n",
       "      <td>...</td>\n",
       "      <td>6.911789e-19</td>\n",
       "      <td>-3.117081e-19</td>\n",
       "      <td>-6.640738e-19</td>\n",
       "      <td>3.794708e-19</td>\n",
       "      <td>-1.029992e-18</td>\n",
       "      <td>-1.707618e-18</td>\n",
       "      <td>-1.761829e-19</td>\n",
       "      <td>3.388132e-19</td>\n",
       "      <td>3.489776e-19</td>\n",
       "      <td>8.402567e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5295</th>\n",
       "      <td>0.220313</td>\n",
       "      <td>0.029904</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.035671</td>\n",
       "      <td>0.017494</td>\n",
       "      <td>0.066387</td>\n",
       "      <td>-0.006452</td>\n",
       "      <td>-0.005851</td>\n",
       "      <td>0.052100</td>\n",
       "      <td>0.031431</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.963112e-19</td>\n",
       "      <td>-3.947174e-19</td>\n",
       "      <td>-6.234162e-19</td>\n",
       "      <td>-5.421011e-19</td>\n",
       "      <td>-1.341700e-18</td>\n",
       "      <td>-2.385245e-18</td>\n",
       "      <td>-1.897354e-19</td>\n",
       "      <td>2.981556e-19</td>\n",
       "      <td>7.589415e-19</td>\n",
       "      <td>5.692061e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5296</th>\n",
       "      <td>0.234396</td>\n",
       "      <td>0.135858</td>\n",
       "      <td>-0.073980</td>\n",
       "      <td>0.022718</td>\n",
       "      <td>-0.037104</td>\n",
       "      <td>-0.010010</td>\n",
       "      <td>0.017729</td>\n",
       "      <td>0.049966</td>\n",
       "      <td>-0.038229</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>...</td>\n",
       "      <td>1.804180e-19</td>\n",
       "      <td>1.639856e-18</td>\n",
       "      <td>1.738112e-18</td>\n",
       "      <td>-4.582448e-19</td>\n",
       "      <td>9.249600e-19</td>\n",
       "      <td>4.404571e-20</td>\n",
       "      <td>1.235398e-18</td>\n",
       "      <td>1.236668e-18</td>\n",
       "      <td>6.894848e-19</td>\n",
       "      <td>7.098136e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>0.294845</td>\n",
       "      <td>-0.051692</td>\n",
       "      <td>0.038916</td>\n",
       "      <td>0.175387</td>\n",
       "      <td>-0.313029</td>\n",
       "      <td>0.133039</td>\n",
       "      <td>0.252425</td>\n",
       "      <td>-0.170509</td>\n",
       "      <td>-0.031680</td>\n",
       "      <td>0.312841</td>\n",
       "      <td>...</td>\n",
       "      <td>1.560658e-19</td>\n",
       "      <td>-2.082007e-18</td>\n",
       "      <td>8.368686e-19</td>\n",
       "      <td>-1.624609e-18</td>\n",
       "      <td>1.394216e-18</td>\n",
       "      <td>-6.516860e-19</td>\n",
       "      <td>-4.133521e-19</td>\n",
       "      <td>-1.080814e-18</td>\n",
       "      <td>6.539094e-19</td>\n",
       "      <td>1.445038e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5298</th>\n",
       "      <td>0.218290</td>\n",
       "      <td>-0.233101</td>\n",
       "      <td>-0.077118</td>\n",
       "      <td>-0.160811</td>\n",
       "      <td>-0.024862</td>\n",
       "      <td>-0.008341</td>\n",
       "      <td>-0.079067</td>\n",
       "      <td>-0.065762</td>\n",
       "      <td>-0.015453</td>\n",
       "      <td>-0.044433</td>\n",
       "      <td>...</td>\n",
       "      <td>2.253108e-19</td>\n",
       "      <td>-3.162821e-18</td>\n",
       "      <td>9.859464e-19</td>\n",
       "      <td>-2.425902e-18</td>\n",
       "      <td>-3.235666e-19</td>\n",
       "      <td>3.218725e-20</td>\n",
       "      <td>-2.771068e-18</td>\n",
       "      <td>2.066760e-19</td>\n",
       "      <td>1.128248e-18</td>\n",
       "      <td>1.517036e-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5299 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       svd0000   svd0001   svd0002   svd0003   svd0004   svd0005   svd0006  \\\n",
       "0     0.270461  0.130039 -0.030061 -0.004413 -0.012367  0.005488 -0.021099   \n",
       "1     0.167226 -0.114772  0.009938 -0.036315  0.001427  0.059813 -0.013987   \n",
       "2     0.255663 -0.191578  0.040965  0.098437 -0.165433  0.070955  0.071593   \n",
       "3     0.125376 -0.105049 -0.015376 -0.009029 -0.042645 -0.075272  0.026641   \n",
       "4     0.320904  0.160357 -0.002871 -0.014655 -0.034423 -0.044569 -0.028687   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5294  0.225822 -0.005949 -0.020814  0.108820 -0.060078 -0.013237 -0.045673   \n",
       "5295  0.220313  0.029904  0.004537  0.035671  0.017494  0.066387 -0.006452   \n",
       "5296  0.234396  0.135858 -0.073980  0.022718 -0.037104 -0.010010  0.017729   \n",
       "5297  0.294845 -0.051692  0.038916  0.175387 -0.313029  0.133039  0.252425   \n",
       "5298  0.218290 -0.233101 -0.077118 -0.160811 -0.024862 -0.008341 -0.079067   \n",
       "\n",
       "       svd0007   svd0008   svd0009  ...       svd0990       svd0991  \\\n",
       "0     0.060785  0.083293  0.032068  ... -2.253108e-18 -3.083200e-18   \n",
       "1    -0.020511  0.045512 -0.116298  ...  3.794708e-19 -4.092863e-18   \n",
       "2     0.018734  0.056828 -0.038015  ... -2.195509e-18 -4.106416e-18   \n",
       "3    -0.000863 -0.028413  0.011569  ... -2.867365e-18 -6.819462e-18   \n",
       "4     0.017214  0.078894  0.015232  ...  1.450120e-18  2.317482e-18   \n",
       "...        ...       ...       ...  ...           ...           ...   \n",
       "5294  0.005723  0.089451 -0.059352  ...  6.911789e-19 -3.117081e-19   \n",
       "5295 -0.005851  0.052100  0.031431  ... -5.963112e-19 -3.947174e-19   \n",
       "5296  0.049966 -0.038229 -0.001414  ...  1.804180e-19  1.639856e-18   \n",
       "5297 -0.170509 -0.031680  0.312841  ...  1.560658e-19 -2.082007e-18   \n",
       "5298 -0.065762 -0.015453 -0.044433  ...  2.253108e-19 -3.162821e-18   \n",
       "\n",
       "           svd0992       svd0993       svd0994       svd0995       svd0996  \\\n",
       "0    -2.683400e-18  3.001885e-18  4.065758e-19 -1.599198e-18 -3.781155e-18   \n",
       "1    -1.694066e-18  1.497554e-18  4.252105e-19  1.355253e-20 -7.411538e-19   \n",
       "2    -5.204170e-18 -3.611748e-18 -6.308701e-18 -1.192622e-18 -3.601584e-18   \n",
       "3     1.782157e-18 -5.746272e-18 -1.228198e-20 -7.559769e-18  1.314172e-18   \n",
       "4     1.246832e-18  6.606857e-19 -1.253609e-18 -1.463673e-18  5.739495e-18   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "5294 -6.640738e-19  3.794708e-19 -1.029992e-18 -1.707618e-18 -1.761829e-19   \n",
       "5295 -6.234162e-19 -5.421011e-19 -1.341700e-18 -2.385245e-18 -1.897354e-19   \n",
       "5296  1.738112e-18 -4.582448e-19  9.249600e-19  4.404571e-20  1.235398e-18   \n",
       "5297  8.368686e-19 -1.624609e-18  1.394216e-18 -6.516860e-19 -4.133521e-19   \n",
       "5298  9.859464e-19 -2.425902e-18 -3.235666e-19  3.218725e-20 -2.771068e-18   \n",
       "\n",
       "           svd0997       svd0998       svd0999  \n",
       "0    -1.256150e-18 -3.662570e-18 -1.541600e-18  \n",
       "1    -6.369688e-19 -1.246832e-18 -1.965116e-19  \n",
       "2    -2.818926e-18 -1.707618e-18 -5.000883e-18  \n",
       "3     5.655639e-18  2.422938e-18  3.423284e-18  \n",
       "4     1.057097e-18 -1.016440e-18  3.652406e-18  \n",
       "...            ...           ...           ...  \n",
       "5294  3.388132e-19  3.489776e-19  8.402567e-19  \n",
       "5295  2.981556e-19  7.589415e-19  5.692061e-19  \n",
       "5296  1.236668e-18  6.894848e-19  7.098136e-19  \n",
       "5297 -1.080814e-18  6.539094e-19  1.445038e-18  \n",
       "5298  2.066760e-19  1.128248e-18  1.517036e-18  \n",
       "\n",
       "[5299 rows x 1000 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_train_dim_reduct, columns=[f\"svd{num:04}\" for num in range(0,X_train_dim_reduct.shape[1])])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf141c",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "We train and evaluate seven different models to understand their baseline performance on our dataset. Each model is assessed based on its accuracy on the training and test sets. The models include:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. K-Nearest Neighbors (KNN)\n",
    "3. Support Vector Machine (SVM)\n",
    "4. Decision Tree\n",
    "5. Random Forest\n",
    "6. AdaBoost\n",
    "7. XGBoost\n",
    "\n",
    "For each model, the training process is followed by an evaluation using accuracy as the primary metric.\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "To optimize the performance of each model, we apply hyperparameter tuning using GridSearchCV.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0955fc8",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b4d3648-5995-4ce0-bac8-fbbe6e254d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Train accuracy: 0.9619\n",
      "Test accuracy: 0.9348\n",
      "Confusion Matrix:\n",
      "[[727   0  85]\n",
      " [  0 618   0]\n",
      " [ 63   0 778]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "lr_clf.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "y_train_pred_lr = lr_clf.predict(X_train_dim_reduct)\n",
    "y_test_pred_lr = lr_clf.predict(X_test__dim_reduct)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_train_pred_lr):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred_lr):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f864918e",
   "metadata": {},
   "source": [
    "Hyper parameter tuning for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e4430dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Best LR Parameters: {'C': 10, 'solver': 'liblinear'}\n",
      "Best LR Score: 0.9598\n",
      "Logistic Regression Comparison:\n",
      "Baseline Train Accuracy: 0.9619, Test Accuracy: 0.9348\n",
      "After Tuning Train Accuracy: 0.9598, Test Accuracy: 0.9498\n",
      "Logistic Regression Tuning Time: 4.20 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "start_time = time.time()\n",
    "lr_grid = GridSearchCV(LogisticRegression(random_state=42, max_iter=1000), \n",
    "                       {'C': [0.1, 1, 10], 'solver': ['liblinear']}, \n",
    "                       cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "lr_grid.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "print(f\"Best LR Parameters: {lr_grid.best_params_}\")\n",
    "print(f\"Best LR Score: {lr_grid.best_score_:.4f}\")\n",
    "\n",
    "print(\"Logistic Regression Comparison:\")\n",
    "print(f\"Baseline Train Accuracy: {accuracy_score(y_train, y_train_pred_lr):.4f}, Test Accuracy: {accuracy_score(y_test, y_test_pred_lr):.4f}\")\n",
    "print(f\"After Tuning Train Accuracy: {lr_grid.best_score_:.4f}, Test Accuracy: {accuracy_score(y_test, lr_grid.best_estimator_.predict(X_test__dim_reduct)):.4f}\")\n",
    "\n",
    "\n",
    "lr_time = time.time() - start_time\n",
    "\n",
    "print(f\"Logistic Regression Tuning Time: {lr_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabe17f",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fa8d32ea-9650-4c2c-8554-2f22793b6882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors (KNN):\n",
      "Train accuracy: 0.9989\n",
      "Test accuracy: 0.9982\n",
      "Confusion Matrix:\n",
      "[[808   0   4]\n",
      " [  0 618   0]\n",
      " [  0   0 841]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "y_train_pred_knn = knn_clf.predict(X_train_dim_reduct)\n",
    "y_test_pred_knn = knn_clf.predict(X_test__dim_reduct)\n",
    "\n",
    "print(\"K-Nearest Neighbors (KNN):\")\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_train_pred_knn):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred_knn):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb12f738",
   "metadata": {},
   "source": [
    "Hyper parameter tuning for KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5dfd9216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best KNN Parameters: {'metric': 'euclidean', 'n_neighbors': 3}\n",
      "Best KNN Score: 0.9942\n",
      "KNN Comparison:\n",
      "Baseline Train Accuracy: 0.9989, Test Accuracy: 0.9982\n",
      "After Tuning Train Accuracy: 0.9942, Test Accuracy: 0.9982\n",
      "KNN Tuning Time: 9.78 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "knn_grid = GridSearchCV(KNeighborsClassifier(), \n",
    "                        {'n_neighbors': [3, 5, 7], 'metric': ['euclidean', 'manhattan']}, \n",
    "                        cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "knn_grid.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "print(f\"Best KNN Parameters: {knn_grid.best_params_}\")\n",
    "print(f\"Best KNN Score: {knn_grid.best_score_:.4f}\")\n",
    "\n",
    "print(\"KNN Comparison:\")\n",
    "print(f\"Baseline Train Accuracy: {accuracy_score(y_train, y_train_pred_knn):.4f}, Test Accuracy: {accuracy_score(y_test, y_test_pred_knn):.4f}\")\n",
    "print(f\"After Tuning Train Accuracy: {knn_grid.best_score_:.4f}, Test Accuracy: {accuracy_score(y_test, knn_grid.best_estimator_.predict(X_test__dim_reduct)):.4f}\")\n",
    "knn_time = time.time() - start_time\n",
    "\n",
    "print(f\"KNN Tuning Time: {knn_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e010b",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c3e38007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine (SVM):\n",
      "Train accuracy: 0.9564\n",
      "Test accuracy: 0.9256\n",
      "Confusion Matrix:\n",
      "[[719   0  93]\n",
      " [  0 618   0]\n",
      " [ 76   0 765]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "y_train_pred_svm = svm_clf.predict(X_train_dim_reduct)\n",
    "y_test_pred_svm = svm_clf.predict(X_test__dim_reduct)\n",
    "\n",
    "print(\"Support Vector Machine (SVM):\")\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_train_pred_svm):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred_svm):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c8aa5d",
   "metadata": {},
   "source": [
    "Hyper parameter tuning for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "99ae2efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best SVM Parameters: {'C': 10, 'kernel': 'rbf'}\n",
      "Best SVM Score: 0.9623\n",
      "SVM Comparison:\n",
      "Baseline Train Accuracy: 0.9564, Test Accuracy: 0.9256\n",
      "After Tuning Train Accuracy: 0.9623, Test Accuracy: 0.9599\n",
      "SVM Tuning Time: 44.52 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "svm_grid = GridSearchCV(SVC(random_state=42), \n",
    "                        {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}, \n",
    "                        cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "svm_grid.fit(X_train_dim_reduct, y_train)\n",
    "print(f\"Best SVM Parameters: {svm_grid.best_params_}\")\n",
    "print(f\"Best SVM Score: {svm_grid.best_score_:.4f}\")\n",
    "\n",
    "print(\"SVM Comparison:\")\n",
    "print(f\"Baseline Train Accuracy: {accuracy_score(y_train, y_train_pred_svm):.4f}, Test Accuracy: {accuracy_score(y_test, y_test_pred_svm):.4f}\")\n",
    "print(f\"After Tuning Train Accuracy: {svm_grid.best_score_:.4f}, Test Accuracy: {accuracy_score(y_test, svm_grid.best_estimator_.predict(X_test__dim_reduct)):.4f}\")\n",
    "svm_time = time.time() - start_time\n",
    "\n",
    "print(f\"SVM Tuning Time: {svm_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fdd32c",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "92602358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "Train accuracy: 0.9989\n",
      "Test accuracy: 0.9978\n",
      "Confusion Matrix:\n",
      "[[808   0   4]\n",
      " [  1 617   0]\n",
      " [  0   0 841]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "y_train_pred_dt = dt_clf.predict(X_train_dim_reduct)\n",
    "y_test_pred_dt = dt_clf.predict(X_test__dim_reduct)\n",
    "\n",
    "print(\"Decision Tree:\")\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_train_pred_dt):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred_dt):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e211f408",
   "metadata": {},
   "source": [
    "Hyper parameter tuning for decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eb0ae10b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Best parameters for Decision Tree: {'max_depth': None, 'min_samples_split': 2}\n",
      "Best score for Decision Tree: 0.9962248115880058\n",
      "Decision Tree Comparison:\n",
      "Baseline Train Accuracy: 0.9989, Test Accuracy: 0.9978\n",
      "After Tuning Train Accuracy: 0.9962, Test Accuracy: 0.9978\n",
      "Decision Tree Tuning Time: 10.34 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "dt_param_grid = {\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt_grid = GridSearchCV(dt, dt_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "dt_grid.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "print(f\"Best parameters for Decision Tree: {dt_grid.best_params_}\")\n",
    "print(f\"Best score for Decision Tree: {dt_grid.best_score_}\")\n",
    "\n",
    "print(\"Decision Tree Comparison:\")\n",
    "print(f\"Baseline Train Accuracy: {accuracy_score(y_train, y_train_pred_dt):.4f}, Test Accuracy: {accuracy_score(y_test, y_test_pred_dt):.4f}\")\n",
    "print(f\"After Tuning Train Accuracy: {dt_grid.best_score_:.4f}, Test Accuracy: {accuracy_score(y_test, dt_grid.best_estimator_.predict(X_test__dim_reduct)):.4f}\")\n",
    "dt_time = time.time() - start_time\n",
    "\n",
    "print(f\"Decision Tree Tuning Time: {dt_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc1e08",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c6ab8fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Train accuracy: 0.8420\n",
      "Test accuracy: 0.8159\n",
      "Confusion Matrix:\n",
      "[[466  73 273]\n",
      " [  0 617   1]\n",
      " [  0  71 770]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_leaf_nodes=10, n_jobs=-1)\n",
    "rf_clf.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "y_train_pred_rf = rf_clf.predict(X_train_dim_reduct)\n",
    "y_test_pred_rf = rf_clf.predict(X_test__dim_reduct)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_train_pred_rf):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred_rf):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021477ea",
   "metadata": {},
   "source": [
    "Hyper parameter tuning for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8e19e043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
      "Best parameters for Random Forest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best score for Random Forest: 0.9984905660377358\n",
      "Random Forest Comparison:\n",
      "Baseline Train Accuracy: 0.8420, Test Accuracy: 0.8159\n",
      "After Tuning Train Accuracy: 0.9985, Test Accuracy: 0.9982\n",
      "Random Forest Tuning Time: 152.10 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf_grid = GridSearchCV(rf, rf_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "rf_grid.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "print(f\"Best parameters for Random Forest: {rf_grid.best_params_}\")\n",
    "print(f\"Best score for Random Forest: {rf_grid.best_score_}\")\n",
    "\n",
    "print(\"Random Forest Comparison:\")\n",
    "print(f\"Baseline Train Accuracy: {accuracy_score(y_train, y_train_pred_rf):.4f}, Test Accuracy: {accuracy_score(y_test, y_test_pred_rf):.4f}\")\n",
    "print(f\"After Tuning Train Accuracy: {rf_grid.best_score_:.4f}, Test Accuracy: {accuracy_score(y_test, rf_grid.best_estimator_.predict(X_test__dim_reduct)):.4f}\")\n",
    "rf_time = time.time() - start_time\n",
    "\n",
    "print(f\"Random Forest Tuning Time: {rf_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c5b46",
   "metadata": {},
   "source": [
    "### AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e91f23eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost:\n",
      "Train accuracy: 0.6580\n",
      "Test accuracy: 0.6684\n",
      "Confusion Matrix:\n",
      "[[617   0 195]\n",
      " [  2 612   4]\n",
      " [552   0 289]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ada_clf = AdaBoostClassifier()\n",
    "ada_clf.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "y_train_pred_ada = ada_clf.predict(X_train_dim_reduct)\n",
    "y_test_pred_ada = ada_clf.predict(X_test__dim_reduct)\n",
    "\n",
    "print(\"AdaBoost:\")\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_train_pred_ada):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred_ada):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_ada))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a172dfbf",
   "metadata": {},
   "source": [
    "Hyper parameter tuning for AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "14e4c411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Best parameters for AdaBoost: {'learning_rate': 1, 'n_estimators': 200}\n",
      "Best score for AdaBoost: 0.7248611185347515\n",
      "AdaBoost Comparison:\n",
      "Baseline Train Accuracy: 0.6580, Test Accuracy: 0.6684\n",
      "After Tuning Train Accuracy: 0.7249, Test Accuracy: 0.7107\n",
      "AdaBoost Tuning Time: 184.71 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "ab_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 1]\n",
    "}\n",
    "ab = AdaBoostClassifier(random_state=42)\n",
    "ab_grid = GridSearchCV(ab, ab_param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "ab_grid.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "print(f\"Best parameters for AdaBoost: {ab_grid.best_params_}\")\n",
    "print(f\"Best score for AdaBoost: {ab_grid.best_score_}\")\n",
    "\n",
    "print(\"AdaBoost Comparison:\")\n",
    "print(f\"Baseline Train Accuracy: {accuracy_score(y_train, y_train_pred_ada):.4f}, Test Accuracy: {accuracy_score(y_test, y_test_pred_ada):.4f}\")\n",
    "print(f\"After Tuning Train Accuracy: {ab_grid.best_score_:.4f}, Test Accuracy: {accuracy_score(y_test, ab_grid.best_estimator_.predict(X_test__dim_reduct)):.4f}\")\n",
    "ab_time = time.time() - start_time\n",
    "\n",
    "print(f\"AdaBoost Tuning Time: {ab_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224250c0",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a6ab6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost:\n",
      "Train accuracy: 0.9989\n",
      "Test accuracy: 0.9982\n",
      "Confusion Matrix:\n",
      "[[808   0   4]\n",
      " [  0 618   0]\n",
      " [  0   0 841]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf.fit(X_train_dim_reduct, y_train)\n",
    "\n",
    "y_train_pred_xgb = xgb_clf.predict(X_train_dim_reduct)\n",
    "y_test_pred_xgb = xgb_clf.predict(X_test__dim_reduct)\n",
    "\n",
    "print(\"XGBoost:\")\n",
    "print(f\"Train accuracy: {accuracy_score(y_train, y_train_pred_xgb):.4f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test, y_test_pred_xgb):.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b83c40",
   "metadata": {},
   "source": [
    "Hyper parameter tuning for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d128623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Admi/anaconda3/lib/python3.11/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGB Parameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100}\n",
      "Best XGB Score: 0.9975\n",
      "XGBoost Comparison:\n",
      "Baseline Train Accuracy: 0.9989, Test Accuracy: 0.9982\n",
      "After Tuning Train Accuracy: 0.9975, Test Accuracy: 0.9982\n",
      "XGBoost Tuning Time: 62.85 seconds.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'), \n",
    "                        {'learning_rate': [0.1, 0.2], 'max_depth': [3, 6], 'n_estimators': [50, 100]}, \n",
    "                        cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "xgb_grid.fit(X_train_dim_reduct, y_train)\n",
    "print(f\"Best XGB Parameters: {xgb_grid.best_params_}\")\n",
    "print(f\"Best XGB Score: {xgb_grid.best_score_:.4f}\")\n",
    "\n",
    "print(\"XGBoost Comparison:\")\n",
    "print(f\"Baseline Train Accuracy: {accuracy_score(y_train, y_train_pred_xgb):.4f}, Test Accuracy: {accuracy_score(y_test, y_test_pred_xgb):.4f}\")\n",
    "print(f\"After Tuning Train Accuracy: {xgb_grid.best_score_:.4f}, Test Accuracy: {accuracy_score(y_test, xgb_grid.best_estimator_.predict(X_test__dim_reduct)):.4f}\")\n",
    "\n",
    "xgb_time = time.time() - start_time\n",
    "\n",
    "print(f\"XGBoost Tuning Time: {xgb_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb9131",
   "metadata": {},
   "source": [
    "# Challenges Faced\n",
    "\n",
    "Applying the model and performing hyper parameter tuning was complex as this was a very large dataset. The implementation of hyper parameter tuning had a very high computational cost, as shown in the results, with models taking up to 184 seconds to run. This made it very difficult to test ad make adjustments to the models and code, as it will take significant time. \n",
    "\n",
    "Another challenge might be fighting overfitting, as many of the models present very high accuracies, which might suggest that the data be biased or that the model is trying to explain too much from the training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a839f66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "baseline_accuracies = {\n",
    "    'Logistic Regression': accuracy_score(y_test, y_test_pred_lr),\n",
    "    'KNN': accuracy_score(y_test, y_test_pred_knn),\n",
    "    'SVM': accuracy_score(y_test, y_test_pred_svm),\n",
    "    'Decision Tree': accuracy_score(y_test, y_test_pred_dt),\n",
    "    'Random Forest': accuracy_score(y_test, y_test_pred_rf),\n",
    "    'AdaBoost': accuracy_score(y_test, y_test_pred_ada),\n",
    "    'XGBoost': accuracy_score(y_test, y_test_pred_xgb)\n",
    "}\n",
    "\n",
    "tuned_accuracies = {\n",
    "    'Logistic Regression': accuracy_score(y_test, lr_grid.best_estimator_.predict(X_test__dim_reduct)),\n",
    "    'KNN': accuracy_score(y_test, knn_grid.best_estimator_.predict(X_test__dim_reduct)),\n",
    "    'SVM': accuracy_score(y_test, svm_grid.best_estimator_.predict(X_test__dim_reduct)),\n",
    "    'Decision Tree': accuracy_score(y_test, dt_grid.best_estimator_.predict(X_test__dim_reduct)),\n",
    "    'Random Forest': accuracy_score(y_test, rf_grid.best_estimator_.predict(X_test__dim_reduct)),\n",
    "    'AdaBoost': accuracy_score(y_test, ab_grid.best_estimator_.predict(X_test__dim_reduct)),\n",
    "    'XGBoost': accuracy_score(y_test, xgb_grid.best_estimator_.predict(X_test__dim_reduct))\n",
    "}\n",
    "\n",
    "tuned_times = {\n",
    "    'Logistic Regression': lr_time,\n",
    "    'KNN': knn_time,\n",
    "    'SVM': svm_time,\n",
    "    'Decision Tree': dt_time,\n",
    "    'Random Forest': rf_time,\n",
    "    'AdaBoost': ab_time,\n",
    "    'XGBoost': xgb_time\n",
    "}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3d537b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                Baseline Acc.   Tuned Acc.      Tuning Time (s)\n",
      "Logistic Regression  0.9348          0.9498          4.202322006225586\n",
      "KNN                  0.9982          0.9982          9.781334161758423\n",
      "SVM                  0.9256          0.9599          44.52327609062195\n",
      "Decision Tree        0.9978          0.9978          10.344090223312378\n",
      "Random Forest        0.8159          0.9982          152.0950710773468\n",
      "AdaBoost             0.6684          0.7107          184.71184396743774\n",
      "XGBoost              0.9982          0.9982          62.84921336174011\n"
     ]
    }
   ],
   "source": [
    "models = [\"Logistic Regression\", \"KNN\", \"SVM\", \"Decision Tree\", \"Random Forest\", \"AdaBoost\", \"XGBoost\"]\n",
    "print(f\"{'Model':<20} {'Baseline Acc.':<15} {'Tuned Acc.':<15} {'Tuning Time (s)':<15}\")\n",
    "for model in models:\n",
    "    print(f\"{model:<20} {baseline_accuracies[model]:<15.4f} {tuned_accuracies[model]:<15.4f} {tuned_times[model]:<15}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9589a013",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: Baseline Accuracy = 0.9348.\n",
      "KNN: Baseline Accuracy = 0.9982.\n",
      "SVM: Baseline Accuracy = 0.9256.\n",
      "Decision Tree: Baseline Accuracy = 0.9978.\n",
      "Random Forest: Baseline Accuracy = 0.8159.\n",
      "AdaBoost: Baseline Accuracy = 0.6684.\n",
      "XGBoost: Baseline Accuracy = 0.9982.\n",
      "\n",
      "The best performing model after hyperparameter tuning is KNN with an accuracy of 0.9982, Tuned Accuracy = 0.9982, Tuned time = 9.78 seconds.\n"
     ]
    }
   ],
   "source": [
    "# Print comparison of baseline vs tuned accuracies\n",
    "for model in baseline_accuracies.keys():\n",
    "    print(f\"{model}: Baseline Accuracy = {baseline_accuracies[model]:.4f}.\")\n",
    "\n",
    "# Identify the best performing model after tuning\n",
    "best_model = max(tuned_accuracies, key=tuned_accuracies.get)\n",
    "print(f\"\\nThe best performing model after hyperparameter tuning is {best_model} with an accuracy of {tuned_accuracies[best_model]:.4f}, Tuned Accuracy = {tuned_accuracies[model]:.4f}, Tuned time = {tuned_times[best_model]:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920a727",
   "metadata": {},
   "source": [
    "## Model Performance Improvements\n",
    "\n",
    "The results reveal significant improvements in accuracy for some models post-tuning, notably for **Logistic Regression** and **SVM**, where accuracy increased from 0.9348 to 0.9498 and from 0.9256 to 0.9599, respectively. These improvements underscore the value of hyperparameter tuning in optimizing model performance, particularly for models that are sensitive to parameter settings.\n",
    "\n",
    "**Random Forest** and **AdaBoost** exhibited the most remarkable performance shifts. Random Forest's accuracy leaped from 0.8159 to 0.9982, while AdaBoost saw a modest increase from 0.6684 to 0.7107. This dramatic improvement for Random Forest suggests that the default parameters were far from optimal and that the model significantly benefited from tuning. For AdaBoost, the gain, although smaller, indicates a positive direction towards optimizing its performance.\n",
    "\n",
    "Conversely, **KNN**, **Decision Tree**, and **XGBoost** maintained the same high level of accuracy before and after tuning, which suggests two possibilities: either the default parameters were already near-optimal, or the range of hyperparameters explored during tuning did not significantly impact these models' performance.\n",
    "\n",
    "## Computational Costs\n",
    "\n",
    "The computational cost associated with tuning varied widely among the models. Notably, **AdaBoost** and **Random Forest** required the most time, with 184.71 and 152.09 seconds, respectively. This significant computational investment yielded substantial accuracy improvements for Random Forest, making the cost worthwhile in this scenario. However, for AdaBoost, the considerable tuning time did not result in a similarly dramatic performance increase, highlighting a less efficient use of computational resources.\n",
    "\n",
    "**SVM** also showed a notable computational cost at 44.52 seconds but delivered a solid performance improvement, indicating a good balance between cost and benefit.\n",
    "\n",
    "In contrast, **Logistic Regression**, despite its relatively low computational cost of 4.20 seconds, achieved a meaningful accuracy improvement, showcasing an efficient tuning process.\n",
    "\n",
    "## Best Model Selection\n",
    "\n",
    "Considering both the accuracy improvements and the computational costs, **Random Forest** stands out as the most improved model due to tuning, achieving the highest accuracy among all models post-tuning. This remarkable improvement demonstrates the model's potential when optimally tuned, despite the higher computational cost.\n",
    "\n",
    "However, if computational efficiency is a priority, **Logistic Regression** offers a compelling alternative, with a significant accuracy improvement and minimal tuning time.\n",
    "\n",
    "## Future Directions\n",
    "\n",
    "The results suggest several avenues for further exploration:\n",
    "\n",
    "- **Exploring Further Hyperparameter Ranges**: For models like KNN, Decision Tree, and XGBoost, which showed no improvement with tuning, exploring a broader or different set of hyperparameters might uncover untapped performance potential.\n",
    "- **Feature Engineering and Selection**: Improving model performance might also be achieved by refining the input features, suggesting a direction for further research and experimentation.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a93b04",
   "metadata": {},
   "source": [
    "# Discussion and Conclusion\n",
    "\n",
    "\n",
    "### Insights Gained and Potential Applications:\n",
    "-The high accuracy achieved by KNN suggests that the dataset contains clear, distinguishable patterns that KNN can exploit effectively, making it highly suitable for applications requiring high precision and reliability.\n",
    "-The insights gained from the model comparison and selection process can inform future projects, emphasizing the value of exploratory data analysis, feature engineering, and rigorous model evaluation.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
